{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aswali.github.io/WNet/\n",
    "\n",
    "There are 2 losses: \n",
    "- Soft Normalized cut loss (after 1st U-net)\n",
    "- Reconstruction loss (end)\n",
    "\n",
    "### Parts of the network\n",
    "\n",
    "Same as U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sak.torch.nn\n",
    "\n",
    "\"\"\" \n",
    "Convolutional block:\n",
    "It follows a two 3x3 convolutional layer, each followed by a batch normalization and a relu activation.\n",
    "This is what happens in each level of the U-net!\n",
    "\"\"\"\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_conv):\n",
    "        super().__init__()\n",
    "        self.n_conv = n_conv\n",
    "        \n",
    "        setattr(self, 'conv0', nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)) # the first conv is different (in_channels)\n",
    "        \n",
    "        for i in range(n_conv):\n",
    "            setattr(self, 'relu'+str(i), nn.LeakyReLU())\n",
    "            setattr(self, 'bn'+str(i), nn.BatchNorm2d(out_channels))\n",
    "            setattr(self, 'dropout'+str(i), nn.Dropout2d(0.25))\n",
    "            if i > 0:\n",
    "                setattr(self, 'conv'+str(i), nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = getattr(self,'conv0')(x)\n",
    "        \n",
    "        for i in range(self.n_conv):\n",
    "            if i > 0:\n",
    "                x = getattr(self, 'conv'+str(i))(x)\n",
    "            x = getattr(self, 'relu'+str(i))(x)\n",
    "            x = getattr(self, 'bn'+str(i))(x)\n",
    "            x = getattr(self, 'dropout'+str(i))(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\"\"\" \n",
    "Encoder block:\n",
    "It consists of an conv_block followed by a max pooling.\n",
    "Here the number of filters doubles and the height and width half after every block.\n",
    "\"\"\"\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_conv):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = conv_block(in_channels, out_channels, n_conv)\n",
    "        self.pool = nn.AvgPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # here x acts also as a skip connection\n",
    "        # p goes down a level of the network\n",
    "        x = self.conv(x)\n",
    "        p = self.pool(x)\n",
    "\n",
    "        # the h and w of p is half of x!\n",
    "        return x, p\n",
    "\n",
    "\"\"\" \n",
    "Decoder block:\n",
    "The decoder block begins with a transpose convolution 2x2, followed by a concatenation with the skip\n",
    "connection from the encoder block. Next comes the conv_block.\n",
    "Here the number filters decreases by half and the height and width doubles.\n",
    "\"\"\"\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_conv):\n",
    "        super().__init__()\n",
    "\n",
    "        # stride allows to double the h and w\n",
    "        # no need of padding as we are obtaining bigger feature matrix\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # the input channels are double because we have the ones coming from the up-conv and the ones coming from the skip\n",
    "        self.conv = conv_block(out_channels*2, out_channels, n_conv)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # the decoder block receives as inputs the up-conv and the skip connection!\n",
    "        x = self.up(x) # up-conv\n",
    "        x = torch.cat([x, skip], axis=1) # concatenation of the skip connection, axis = 1 because the concatenation is done in the number of channels\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# comprovaci贸n conv_block:\n",
    "c = conv_block(1, 64, 2)\n",
    "inputs = torch.rand(2, 1, 512, 512)\n",
    "y = c(inputs)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 256, 256]) torch.Size([2, 64, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# comprovaci贸n encoder_block:\n",
    "e = encoder_block(1, 64,2)\n",
    "inputs = torch.rand(2, 1, 512, 512)\n",
    "y,p = e(inputs)\n",
    "print(p.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# comprovaci贸n decoder_block:\n",
    "d = decoder_block(64, 32,2)\n",
    "inputs = torch.rand(2, 64, 256, 256)\n",
    "skip = torch.rand(2, 32, 512, 512)\n",
    "y = d(inputs, skip)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basenet(nn.Module):\n",
    "    def __init__(self, n_channels, i_channels, n_levels, n_conv):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_levels = n_levels\n",
    "        \n",
    "        \"\"\" Encoder \"\"\"\n",
    "        \n",
    "        # list with the operations of the encoder\n",
    "        operations_encoder = [encoder_block(i_channels,n_channels, n_conv)]\n",
    "        \n",
    "        for i in range(1,n_levels-1):\n",
    "            operations_encoder.append(encoder_block(n_channels*2**(i-1), n_channels * 2**i, n_conv))\n",
    "        for i in range(len(operations_encoder)):\n",
    "            setattr(self, 'e'+str(i),operations_encoder[i])\n",
    "        \n",
    "        \"\"\" Bottleneck: bottom part of the diagram \"\"\"\n",
    "        # just another convolution block\n",
    "        self.b = conv_block(n_channels*2**(n_levels-2), n_channels*2**(n_levels-1), n_conv)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        \n",
    "        # list with the operations of the encoder\n",
    "        operations_decoder = [decoder_block(n_channels*2**i, n_channels * 2**(i-1), n_conv) for i in range(n_levels-1,0,-1)]\n",
    "        for i in range(len(operations_decoder)):\n",
    "            setattr(self, 'd'+str(i),operations_decoder[i])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        \n",
    "        conv0_result = self.e0(x)\n",
    "        p_outputs = [conv0_result[1]]\n",
    "        s_outputs = [conv0_result[0]]\n",
    "        \n",
    "        for i in range(1,self.n_levels-1):\n",
    "            conv_op = getattr(self,'e'+str(i)) \n",
    "            conv_result = conv_op(p_outputs[i-1]) \n",
    "            p_outputs.append(conv_result[1]) \n",
    "            s_outputs.append(conv_result[0]) \n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p_outputs[self.n_levels-2])\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d_outputs = [self.d0(b, s_outputs[self.n_levels-2])]\n",
    "        for i in range(1,self.n_levels-1):\n",
    "            d_outputs.append(getattr(self,'d'+str(i))(d_outputs[i-1], s_outputs[-(i+1)]))\n",
    "\n",
    "        return d_outputs[self.n_levels-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unet(nn.Module):\n",
    "    def __init__(self, n_channels, i_channels, f_channels, n_levels, n_conv):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.basenet = basenet(n_channels, i_channels, n_levels, n_conv)\n",
    "        \n",
    "        \"\"\" Classifier: last convolution 1x1 \"\"\"\n",
    "        self.outputs = nn.Conv2d(n_channels, f_channels, kernel_size=1, padding=0)\n",
    "        \n",
    "        \"\"\" Sigmoid \"\"\"\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        ''' Linear regression '''\n",
    "        #self.lr = nn.MaxPool2d((256,1))\n",
    "        self.lr = sak.torch.nn.SoftArgmaxAlongAxis(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.basenet(x)\n",
    "        \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(x)\n",
    "        outputs = self.sigmoid(outputs)\n",
    "        outputs = self.lr(outputs) # linear regresion\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "# comprovaci贸n:\n",
    "n_levels = 7\n",
    "n_channels = 64\n",
    "i_channels = 1\n",
    "f_channels = 1\n",
    "n_conv = 2\n",
    "inputs = torch.randn((2, 1, 256, 512))\n",
    "\n",
    "model = unet(n_channels,i_channels, f_channels, n_levels, n_conv)\n",
    "y = model(inputs)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build W-net from 2 U-nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wnet(nn.Module):\n",
    "    def __init__(self, n_channels, i_channels, f_channels, n_levels, n_conv):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.encoder = basenet(n_channels, i_channels, n_levels, n_conv)\n",
    "        \n",
    "        \"\"\" Middle:  softmax activation function \"\"\"\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.decoder = basenet(n_channels, n_channels, n_levels, n_conv)\n",
    "        \n",
    "        \"\"\" Classifier: last convolution 1x1 \"\"\"\n",
    "        self.outputs = nn.Conv2d(n_channels, f_channels, kernel_size=1, padding=0)\n",
    "        \n",
    "        \"\"\" Sigmoid \"\"\"\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        ''' Linear regression '''\n",
    "        self.lr = nn.MaxPool2d((256,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.softmax(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(x)\n",
    "        outputs = self.sigmoid(outputs)\n",
    "        outputs = self.lr(outputs) # linear regresion\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp240\\AppData\\Local\\Temp/ipykernel_23024/4003982872.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "# comprovaci贸n:\n",
    "n_levels = 7\n",
    "n_channels = 64\n",
    "i_channels = 1\n",
    "f_channels = 1\n",
    "n_conv = 2\n",
    "inputs = torch.randn((2, 1, 256, 512))\n",
    "\n",
    "model = wnet(n_channels,i_channels, f_channels, n_levels, n_conv)\n",
    "y = model(inputs)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
